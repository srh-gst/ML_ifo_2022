{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeches I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "from string import digits, punctuation\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(\"../data/speeches/\")\n",
    "corpus = []\n",
    "for f in folder.glob(\"R0*\"):\n",
    "    try:\n",
    "        text = f.read_text(encoding=\"utf8\")\n",
    "        corpus.append(text)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = nltk.corpus.stopwords.words('english')\n",
    "_stemmer = nltk.snowball.SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    \"\"\"Return tokens of text deprived of numbers and interpunctuation.\"\"\"\n",
    "    text = text.translate(str.maketrans({p: \"\" for p in digits + punctuation}))\n",
    "    return [_stemmer.stem(t) for t in nltk.word_tokenize(text.lower())]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=_stopwords, tokenizer=tokenize_and_stem, ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = Path(\"../output/speech_matrix.pk\")\n",
    "with open(FNAME, \"wb\") as ouf:\n",
    "    dump(tfidf_matrix, ouf)\n",
    "\n",
    "terms = pd.DataFrame(terms)\n",
    "terms.columns = [\"terms\"]\n",
    "terms.to_csv(Path('../output/terms.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeches II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(\"../output/speech_matrix.pk\"), \"rb\") as inf:\n",
    "    tfidf_matrix = load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_matrix = linkage(tfidf_matrix.todense(), metric=\"cosine\", method=\"complete\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.show()\n",
    "fig.savefig(Path(\"../output/speeches_dendrogram.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file into one list and strip whitespace characters at end\n",
    "FNAME = Path(\"../data/Stellenanzeigen.txt\")\n",
    "with open(FNAME, \"r\", encoding=\"utf8\") as inf:\n",
    "    corpus = [x.rstrip() for x in inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficulties:\n",
    "# a) Newspaper name and date in same line, ad text in next\n",
    "# b) Some ads contain a title, thus some entries spend 2 lines and others 3 lines\n",
    "# !! Therefore multiple ways !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE SPLIT WAY\n",
    "# Idea: Join all entries on _same_ separator as that for newspaper and date, and then\n",
    "# split on the double separator generated by the empty line between entries\n",
    "SEP = \", \"\n",
    "temp = SEP.join(corpus).split(SEP*2)\n",
    "paper, date, text = zip(*[e.split(SEP, 3) for e in temp])\n",
    "\n",
    "assert(len(paper) == len(date) == len(text) == 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE ITERATIVE WAY\n",
    "# Idea: Iterate over each line and decide:\n",
    "# If a line ends with a year, it's meta information (title and date), but\n",
    "# if line is empty, conclude the entry, otherwise it's still text\n",
    "\n",
    "paper = []\n",
    "date = []\n",
    "text = []\n",
    "new_entry = []\n",
    "for line in corpus:\n",
    "    if line and line[-4:].isdigit():  # Line is header\n",
    "        new_paper, new_date = line.rsplit(\", \", 1)\n",
    "        paper.append(new_paper)\n",
    "        date.append(new_date)\n",
    "    elif line:  # Line is other content\n",
    "        new_entry.append(line)\n",
    "    else:  # Line is empty\n",
    "        new_entry = \" \".join(new_entry)\n",
    "        text.append(new_entry)\n",
    "        new_entry = []\n",
    "# Also add the last entry\n",
    "text.append(\" \".join(new_entry))\n",
    "\n",
    "assert(len(paper) == len(date) == len(text) == 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into DataFrame\n",
    "df = pd.DataFrame({\"newspaper\": paper, \"date\": date, \"text\": text})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime\n",
    "# Possibble difficulty: pandas' date parser only works \n",
    "# when the locale is correct (there are other ways, too)\n",
    "locale.setlocale(locale.LC_ALL, 'de_CH.utf8')  # On Unix-systems use de_CH.utf8\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%d. %B %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column decade\n",
    "df[\"decade\"] = (df[\"date\"].dt.year//10)*10  # // stands for integer division\n",
    "print(df[\"decade\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of words (not characters) for each text\n",
    "df[\"length\"] = df[\"text\"].str.split().apply(len)  # One might want to clean first, but not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)[\"length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS PLOT\n",
    "# Aggregate by decade and then plot\n",
    "grouped_dec = df.groupby(\"decade\")[\"length\"].mean()\n",
    "fig, ax = plt.subplots()\n",
    "grouped_dec.plot(legend=None, ax=ax)\n",
    "ax.set(xlabel=\"Decade\", ylabel=\"Average length\")\n",
    "plt.savefig(Path(\"../output/job_ads.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN PLOT\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=df, x=\"decade\", y=\"length\", ax=ax)\n",
    "ax.set(xlabel=\"Decade\", ylabel=\"Average length\")\n",
    "plt.savefig(Path(\"../output/job_ads.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"decade\")[\"length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: pandas' sum() after groupby can easily concatenate text, but\n",
    "# you need to add a blank beforehand\n",
    "df[\"text\"] += \" \"\n",
    "grouped_corp = df.groupby(\"decade\")[\"text\"].sum()\n",
    "print(grouped_corp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity use CountVectorizer\n",
    "# Difficulty: There are English texts too, but you can only\n",
    "# use one stemmer\n",
    "\n",
    "remove = digits + punctuation + \"‘’“”„§\"\n",
    "\n",
    "_stemmer = nltk.snowball.SnowballStemmer('german')\n",
    "_stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "_stopwords.extend([\"fur\", \"unt\", \"sowi\"])\n",
    "_stopwords.extend(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    \"\"\"Return tokens of text deprived of numbers and interpunctuation.\"\"\"\n",
    "    text = text.translate(str.maketrans({p: \"\" for p in remove}))\n",
    "    return [_stemmer.stem(t) for t in nltk.word_tokenize(text.lower())]\n",
    "\n",
    "_stopwords = tokenize_and_stem(\" \".join(_stopwords))\n",
    "CV = CountVectorizer(stop_words=_stopwords, tokenizer=tokenize_and_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = CV.fit_transform(grouped_corp.values).todense()\n",
    "count_df = pd.DataFrame(count_matrix, index=grouped_corp.index,\n",
    "                        columns=CV.get_feature_names())\n",
    "count_df = count_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for decade in count_df.columns:\n",
    "    terms = count_df.sort_values(decade).tail().index\n",
    "    print(decade, \":\", \" - \".join(terms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
